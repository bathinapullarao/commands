Kind: Service, ServiceAccount, ingress, Deploymnet, ConfigMap, StorageClass, StatefulSet, HorizontalPodAutoscaler, PersistentVolume, PersistentVolumeClaim, PodDisruptionBudget,
      Role, RoleBinding, ClusterRole, ClusterRoleBinding, DaemonSet,  
pdb(pod distribution budget): A Pod Disruption Budget (PDB) allows you to limit the disruption to your application when its pods need to be rescheduled for some reason 
                              such as upgrades or routine maintenance work on the Kubernetes nodes.
hpa(HorizontalPodAutoscaler): A HorizontalPodAutoscaler (HPA for short) automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of 
                              automatically scaling the workload to match demand.Horizontal scaling means that the response to increased load is to deploy more Pods. 
                              This is different from vertical scaling, which for Kubernetes would mean assigning more resources (for example: memory or CPU) to the Pods that 
                              are already running for the workload.
nginx ingress controller:     A HorizontalPodAutoscaler (HPA for short) automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of 
                              automatically scaling the workload to match demand.Horizontal scaling means that the response to increased load is to deploy more Pods. 
                              This is different from vertical scaling, which for Kubernetes would mean assigning more resources (for example: memory or CPU) to the Pods 
                              that are already running for the workload.
nfs(network file sharing)(PersistentVolume)(PersistentVolumeClaim): is a request for storage by a user.It is similar to a Pod.
                              Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory).
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
AWS-------EKS-------AWS-------EKS-------AWS-------EKS-------AWS-------EKS-------AWS-------EKS-------AWS-------EKS-------AWS-------EKS-------
Prerequisites
AWS CLI installed and configured (aws configure)
kubectl installed
eksctl (optional but helpful for EKS management)
aws configure  <--Ensure you are logged into AWS
aws configure --profile my-profile <--if using a specific profile
aws eks update-kubeconfig --region <AWS_REGION> --name <EKS_CLUSTER_NAME>  <-- to update your kubeconfig with your EKS cluster ~/.kube/config
eg:  aws eks update-kubeconfig --region us-west-2 --name my-cluster
#below command adds the cluster configuration to ~/.kube/config
aws eks update-kubeconfig --region us-west-2 --name my-cluster --profile my-profile   <--using a different AWS profile
kubectl config get-contexts  <--List available contexts
kubectl config current-context  <--Check the current context
kubectl config use-context <CONTEXT_NAME> <--Switch to a different EKS cluster
eg: kubectl config use-context arn:aws:eks:us-west-2:123456789:cluster/my-other-cluster
kubectl config set-context --current --namespace=my-namespace <--By default,kubectl uses the default namespace. Set a preferred namespace for your cluster
kubectl config view --minify | grep namespace  <--Verify the namespace
---------------------------------------------------------------------------------------------------------
aws eks update-kubeconfig  <--If you need to manage multiple EKS clusters, repeat aws eks update-kubeconfig for each cluster.
export KUBECONFIG=~/.kube/config:/path/to/another-kubeconfig  <--Alternatively, you can merge multiple kubeconfig files
kubectl cluster-info  <--Check cluster info
kubectl get nodes   <--List nodes
---------------------------------------------------------------------------------------------------------
GCP-----GKE------GCP-----GKE------GCP-----GKE------GCP-----GKE------GCP-----GKE------GCP-----GKE------GCP-----GKE------GCP-----GKE------
### (~/.kube/config) kubectl configuration to determine which Kubernetes cluster to deploy to.
###if you have multiple projects follow below steps
gcloud auth login   <--Ensure you are authenticated with GCP
gcloud auth activate-service-account --key-file=<key-file.json>      <--using service account credentials
gcloud container clusters get-credentials <CLUSTER_NAME> --region <REGION> --project <PROJECT_ID>   <--For each project and cluster, run
eg: 
gcloud container clusters get-credentials my-cluster-1 --region europe-west2 --project my-project-1
gcloud container clusters get-credentials my-cluster-2 --region us-central1 --project my-project-2  <--This updates ~/.kube/config with the new cluster contexts
kubectl config get-contexts    <---kubectl config get-contexts
----------------------------------------------------------------------------------------------------------
CURRENT   NAME                          CLUSTER            AUTHINFO            NAMESPACE
*         gke_my-project-1_europe-west2_my-cluster-1  gke_my-project-1_europe-west2_my-cluster-1  my-user
          gke_my-project-2_us-central1_my-cluster-2   gke_my-project-2_us-central1_my-cluster-2   my-user
----------------------------------------------------------------------------------------------------------
kubectl config use-context <context-name>    <--to switch to a different cluster
kubectl config use-context gke_my-project-2_us-central1_my-cluster-2   <--Switch context
#kubectl config current-context   <--can check your current cluster context
kubectl config set-context --current --namespace=my-namespace <--By default, kubectl uses the default namespace. To set a preferred namespace for a context
kubectl config view --minify | grep namespace   <--Verify the namespace
----------------------------------------------------------------------------------------------------------
#Instead of modifying ~/.kube/config, you can use separate kubeconfig files.
export KUBECONFIG=/path/to/kubeconfig1         <--Set environment variable for a specific file
kubectl config use-context gke_my-project-1_europe-west2_my-cluster-1
export KUBECONFIG=~/.kube/config:/path/to/other/kubeconfig   <--Merge multiple kubeconfig files
kubectl cluster-info   <--Check which cluster you’re connected to
kubectl config current-context  <--Check active context


kubectl run hello-node --image=hello-node:v1 --port=8080           <--kubectl means cube control
kubectl describe pod
kubectl delete pod hello-node
## Test to ensure the version you installed is up-to-date:
kubectl version --client
kubectl version
## By default, kubectl configuration is located at ~/.kube/config.Check that kubectl is properly configured by getting the cluster state:
kubectl cluster-info
kubectl get all          <--shows all the componets inside cluster
kubectl get nodes        <--To list all the nodes in the cluster
kubectl get pods --all-namespaces         <-- list all the pods running inside your cluster
kubectl get pods --all-namespaces -o wide <--to check where it is running
------------Namespace------------------------------------
kubectl create –f namespace.yml --------->         command to create a namespace
kubectl get namespace ----------------->           list all the available namespace.
kubectl get namespace <Namespace name> ------->    get a particular namespace whose name is specified in the command
kubectl describe namespace <Namespace name> ---->  describe the complete details about the service.
kubectl delete namespace <Namespace name>------->  delete a particular namespace present in the cluster.
---------------------------------------------------------
kubectl create service nodeport nameOfservice(nginx) --tcp=80:80      <--name of service should be same as deploymnet
kubectl get service
kubectl create -h        <---help on creaing
kubectl create deployment nginx-deploy --image=nginx
kubectl get deployment
kubectl expose deployment/my-nginx
kubectl get pod  (or) kubectl get po        <--pod will be ready after deploymnet
kubectl get pods -l app=nginx
kubectl get replicaset   <--manages the replicas of a pod
kubectl get rs           <--manages the replicas of a pod
kubectl edit deployment nginx-deploy  <--you may edit version .. etc
kubectl get replicaset
kubectl logs podname                   <--get logs
kubectl create deployment mongo-deploy --image=mongo
kubectl create -f deploymnet.yaml
kubectl describe pod podname            <--get aditional information
kubectl exec -it podname -- /bin/bash   <--enter in to terminal for debugging
exit                                    <--to exit from container
kubectl get deployment                  <--check the name of deploymnet
kubectl delete deployment nameofdeploymnet
kubectl delete -f deployment.yml
kubectl delete -f service.yml
kubectl create deployment name image option1 option2 
kubectl apply -f config-file.yml        <--apply the configuration
kubectl describe service sevicename     <-- can see all service details like example port forwording..etc
kubectl get svc my-nginx
kubectl describe deployments
kubectl get pod -o wide                 <--get internal ip details of container
kubectl get deployment deploymnetname -o yaml > result.yml   <--can see etcd in yaml format
echo -n 'username' | base64             <-- to encript username or password to keep in secret
kubectl apply -f secret.yml
kubectl get secret
//kubectl delete deployments,svc my-nginx; kubectl create -f ./nginx-secure-app.yamL

#Delete node from cluster.
kubectl drain <node-name>
kubectl drain <node-name> --ignore-daemonsets --delete-local-data
kubectl delete node <node-name>
kubeadm reset
-------------------------
kubeadm reset
echo 1 > /proc/sys/net/ipv4/ip_forward    <--ip_forward content with 1
kubeadm version
kubeadm upgrade plan
kubeadm upgrade apply v1.20.x

kubeadm upgrade node
kubeadm upgrade apply
kubeadm certs check-expiration
kubeadm certs renew apiserver --use-api &
kubectl certificate approve kubeadm-cert-kube-apiserver-ld526
systemctl daemon-reload
systemctl restart kubelet
---------Migration with node pools----------
kubectl cordon <Node name>
kubectl drain <Node nae> --force
gcloud container node-pools delete default-pool
--------
kubectl taint nodes backup1=backups-only:NoSchedule
---------------------------
minikube start
minikube status
--------------------------
kind: Service
kind: Endpoints

-------------------------------------------------------------------
Install kustomize
sudo -i
curl -s "https://raw.githubusercontent.com/\
> kubernetes-sigs/kustomize/master/hack/install_kustomize.sh"  | bash

mv kustomize /usr/local/bin/kustomize

uat only
kustomize build kustomize/overlay/dev/uat | kubectl apply -f -
pro only
kustomize build kustomize/overlay/dev/pro | kubectl apply -f -

* Note: By adding nfs to base, canary and default can no longer be deployed together. Deploy individually as above.
kustomize build kustomize/overlay/dev | kubectl apply -f -


kind: 
ConfigMap
Service
StatefulSet
StorageClass
Deployment
Ingress
Secret
Role
ClusterRole
RoleBinding
ClusterRoleBinding
ServiceAccount

resource "kubernetes_config_map" "ai-gateway-config" {
  metadata {
    name      = "ai-gateway-config"
    namespace = "api"
  }

  data = {
    # Define your config key-value pairs here
    "SPRING_PROFILES_ACTIVE" = "app10-b"
  }
}

resource "kubernetes_deployment" "ai-gateway" {
  metadata {
    name      = "ai-gateway"
    namespace = "api"
  }
  spec {
    replicas = 1

    selector {
      match_labels = {
        app = "ai-gateway"
      }
    }

    template {
      metadata {
        labels = {
          app = "ai-gateway"
        }
      }

      spec {
        container {
          name              = "ai-gateway"
          image             = "gcr.io/purplegrid/aigateway:app10-b"
          image_pull_policy = "Always"
          port {
            container_port = 80
          }


          env {
            # Inject config map data as environment variables
            name = "SPRING_PROFILES_ACTIVE"
            value_from {
              config_map_key_ref {
                name = "ai-gateway-config"
                key  = "SPRING_PROFILES_ACTIVE"
              }
            }
          }
          resources {
            limits = {
              cpu    = "2"
              memory = "8Gi"
            }
            requests = {
              cpu    = "1"
              memory = "6Gi"
            }
          }
        }
      }
    }
  }
}
------------------------
apiVersion: cloud.google.com/v1
kind: BackendConfig
metadata:
  name: ai-gatewaybackendconfig
  namespace: api
spec:
  securityPolicy:
    name: "allowtheseguysonly"
  timeoutSec: 300
  healthCheck:
    checkIntervalSec: 15
    timeoutSec: 15
    healthyThreshold: 1
    unhealthyThreshold: 10
    type: HTTP
    requestPath: /aiservices/actuator/health
---------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: ai-gateway-service
  namespace: api
  annotations:
    cloud.google.com/backend-config: '{"default": "ai-gatewaybackendconfig"}'
spec:
  type: NodePort
  selector:
    app: ai-gateway
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
-----------------------------------------------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app10-b-external-lb
  namespace: api
  annotations:
    #kubernetes.io/spec.ingressClassName: "gce"
    kubernetes.io/ingress.allow-http: "true"
    networking.gke.io/v1beta1.FrontendConfig: lb-frontend-config
spec:
  defaultBackend:
    service:
      name: jello-service
      port:
        number: 80
  rules:
  - http:
      paths:
      - path: /aigateway/*
        pathType: ImplementationSpecific
        backend:
          service:
            name: analytics-service
            port:
              number: 80
