Ref: https://phoenixnap.com/kb/how-to-install-kubernetes-on-centos
Ref: https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/
Ref: https://kubernetes.io/docs/reference/kubectl/kubectl/
Ref: https://kubernetes.io/docs/reference/kubectl/cheatsheet/
https://www.cnblogs.com/zhenyuyaodidiao/p/6502171.html
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Kind: Service, ServiceAccount, ingress, Deploymnet, ConfigMap, StorageClass, StatefulSet, HorizontalPodAutoscaler, PersistentVolume, PersistentVolumeClaim, PodDisruptionBudget,
      Role, RoleBinding, ClusterRole, ClusterRoleBinding, DaemonSet,  
pdb(pod distribution budget): A Pod Disruption Budget (PDB) allows you to limit the disruption to your application when its pods need to be rescheduled for some reason 
                              such as upgrades or routine maintenance work on the Kubernetes nodes.
hpa(HorizontalPodAutoscaler): A HorizontalPodAutoscaler (HPA for short) automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of 
                              automatically scaling the workload to match demand.Horizontal scaling means that the response to increased load is to deploy more Pods. 
                              This is different from vertical scaling, which for Kubernetes would mean assigning more resources (for example: memory or CPU) to the Pods that 
                              are already running for the workload.
nginx ingress controller:     A HorizontalPodAutoscaler (HPA for short) automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of 
                              automatically scaling the workload to match demand.Horizontal scaling means that the response to increased load is to deploy more Pods. 
                              This is different from vertical scaling, which for Kubernetes would mean assigning more resources (for example: memory or CPU) to the Pods 
                              that are already running for the workload.
nfs(network file sharing)(PersistentVolume)(PersistentVolumeClaim): is a request for storage by a user.It is similar to a Pod.
                              Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory).
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

kubectl run hello-node --image=hello-node:v1 --port=8080           <--kubectl means cube control
kubectl describe pod
kubectl delete pod hello-node
## Test to ensure the version you installed is up-to-date:
kubectl version --client
kubectl version
## By default, kubectl configuration is located at ~/.kube/config.Check that kubectl is properly configured by getting the cluster state:
kubectl cluster-info
kubectl get all          <--shows all the componets inside cluster
kubectl get nodes        <--To list all the nodes in the cluster
kubectl get pods --all-namespaces         <-- list all the pods running inside your cluster
kubectl get pods --all-namespaces -o wide <--to check where it is running
------------Namespace------------------------------------
kubectl create –f namespace.yml --------->         command to create a namespace
kubectl get namespace ----------------->           list all the available namespace.
kubectl get namespace <Namespace name> ------->    get a particular namespace whose name is specified in the command
kubectl describe namespace <Namespace name> ---->  describe the complete details about the service.
kubectl delete namespace <Namespace name>------->  delete a particular namespace present in the cluster.
---------------------------------------------------------
kubectl create service nodeport nameOfservice(nginx) --tcp=80:80      <--name of service should be same as deploymnet
kubectl get service
kubectl create -h        <---help on creaing
kubectl create deployment nginx-deploy --image=nginx
kubectl get deployment
kubectl expose deployment/my-nginx
kubectl get pod  (or) kubectl get po        <--pod will be ready after deploymnet
kubectl get pods -l app=nginx
kubectl get replicaset   <--manages the replicas of a pod
kubectl get rs           <--manages the replicas of a pod
kubectl edit deployment nginx-deploy  <--you may edit version .. etc
kubectl get replicaset
kubectl logs podname                   <--get logs
kubectl create deployment mongo-deploy --image=mongo
kubectl create -f deploymnet.yaml
kubectl describe pod podname            <--get aditional information
kubectl exec -it podname -- /bin/bash   <--enter in to terminal for debugging
exit                                    <--to exit from container
kubectl get deployment                  <--check the name of deploymnet
kubectl delete deployment nameofdeploymnet
kubectl delete -f deployment.yml
kubectl delete -f service.yml
kubectl create deployment name image option1 option2 
kubectl apply -f config-file.yml        <--apply the configuration
kubectl describe service sevicename     <-- can see all service details like example port forwording..etc
kubectl get svc my-nginx
kubectl describe deployments
kubectl get pod -o wide                 <--get internal ip details of container
kubectl get deployment deploymnetname -o yaml > result.yml   <--can see etcd in yaml format
echo -n 'username' | base64             <-- to encript username or password to keep in secret
kubectl apply -f secret.yml
kubectl get secret
//kubectl delete deployments,svc my-nginx; kubectl create -f ./nginx-secure-app.yamL

#Delete node from cluster.
kubectl drain <node-name>
kubectl drain <node-name> --ignore-daemonsets --delete-local-data
kubectl delete node <node-name>
kubeadm reset
-------------------------
kubeadm reset
echo 1 > /proc/sys/net/ipv4/ip_forward    <--ip_forward content with 1
kubeadm version
kubeadm upgrade plan
kubeadm upgrade apply v1.20.x

kubeadm upgrade node
kubeadm upgrade apply
kubeadm certs check-expiration
kubeadm certs renew apiserver --use-api &
kubectl certificate approve kubeadm-cert-kube-apiserver-ld526
systemctl daemon-reload
systemctl restart kubelet
---------Migration with node pools----------
kubectl cordon <Node name>
kubectl drain <Node nae> --force
gcloud container node-pools delete default-pool
--------
kubectl taint nodes backup1=backups-only:NoSchedule
---------------------------
minikube start
minikube status
--------------------------
kind: Service
kind: Endpoints

-------------------------------------------------------------------
Install kustomize
sudo -i
curl -s "https://raw.githubusercontent.com/\
> kubernetes-sigs/kustomize/master/hack/install_kustomize.sh"  | bash

mv kustomize /usr/local/bin/kustomize

uat only
kustomize build kustomize/overlay/dev/uat | kubectl apply -f -
pro only
kustomize build kustomize/overlay/dev/pro | kubectl apply -f -

* Note: By adding nfs to base, canary and default can no longer be deployed together. Deploy individually as above.
kustomize build kustomize/overlay/dev | kubectl apply -f -


kind: 
ConfigMap
Service
StatefulSet
StorageClass
Deployment
Ingress
Secret
Role
ClusterRole
RoleBinding
ClusterRoleBinding
ServiceAccount

resource "kubernetes_config_map" "ai-gateway-config" {
  metadata {
    name      = "ai-gateway-config"
    namespace = "api"
  }

  data = {
    # Define your config key-value pairs here
    "SPRING_PROFILES_ACTIVE" = "app10-b"
  }
}

resource "kubernetes_deployment" "ai-gateway" {
  metadata {
    name      = "ai-gateway"
    namespace = "api"
  }
  spec {
    replicas = 1

    selector {
      match_labels = {
        app = "ai-gateway"
      }
    }

    template {
      metadata {
        labels = {
          app = "ai-gateway"
        }
      }

      spec {
        container {
          name              = "ai-gateway"
          image             = "gcr.io/purplegrid/aigateway:app10-b"
          image_pull_policy = "Always"
          port {
            container_port = 80
          }


          env {
            # Inject config map data as environment variables
            name = "SPRING_PROFILES_ACTIVE"
            value_from {
              config_map_key_ref {
                name = "ai-gateway-config"
                key  = "SPRING_PROFILES_ACTIVE"
              }
            }
          }
          resources {
            limits = {
              cpu    = "2"
              memory = "8Gi"
            }
            requests = {
              cpu    = "1"
              memory = "6Gi"
            }
          }
        }
      }
    }
  }
}
------------------------
apiVersion: cloud.google.com/v1
kind: BackendConfig
metadata:
  name: ai-gatewaybackendconfig
  namespace: api
spec:
  securityPolicy:
    name: "allowtheseguysonly"
  timeoutSec: 300
  healthCheck:
    checkIntervalSec: 15
    timeoutSec: 15
    healthyThreshold: 1
    unhealthyThreshold: 10
    type: HTTP
    requestPath: /aiservices/actuator/health
---------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: ai-gateway-service
  namespace: api
  annotations:
    cloud.google.com/backend-config: '{"default": "ai-gatewaybackendconfig"}'
spec:
  type: NodePort
  selector:
    app: ai-gateway
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
-----------------------------------------------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app10-b-external-lb
  namespace: api
  annotations:
    #kubernetes.io/spec.ingressClassName: "gce"
    kubernetes.io/ingress.allow-http: "true"
    networking.gke.io/v1beta1.FrontendConfig: lb-frontend-config
spec:
  defaultBackend:
    service:
      name: jello-service
      port:
        number: 80
  rules:
  - http:
      paths:
      - path: /aigateway/*
        pathType: ImplementationSpecific
        backend:
          service:
            name: analytics-service
            port:
              number: 80
