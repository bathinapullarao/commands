When k8s app is not opening troubleshoot like below
kubectl get pods -n prod  <--in prod namespace If pods are not READY, app will not open.
kubectl get svc -n prod   <--Expected:myapp-service   NodePort   8080:30080/TCP <--where NodePort is 30080
kubectl get ingress -n prod
kubectl logs -n prod -l app=myapp --tail=50
kubectl describe pod <pod-name> -n <namespace>
-------------------------------------------------------------------------------------------------------
comman errors
ImagePullBackOff          <-- image not able to pull from registory

**Kubernetes (k8s) manifest file is a YAML or JSON configuration file that tells Kubernetes what resources to create and how to configure them in your cluster.
managing and scaling applications using Kubernetes, including the use of deployments, statefulsets, and daemonsets?
-------------------------------------------------------------------------------------------------------------------------------------
Ingress routing two types
1.***Path-based routing (recommended)
Single domain ‚Üí different paths
Example:
http://myapp.local/myapp ‚Üí myapp-service
http://myapp.local/orders ‚Üí orderservice

2.****Host-based routing (microservice subdomains)
Different hostnames:
myapp.local ‚Üí myapp
orders.myapp.local ‚Üí orderservice
-----------------------------------------------------------------------------------------------------------
| Cloud | Concept        | Resource Type / Term                    |
|-------|----------------|-----------------------------------------|
| AWS   | EKS Node Group | `aws_eks_node_group`                    |
| GCP   | GKE Node Pool  | `google_container_node_pool`            |
| Azure | AKS Node Pool  | `azurerm_kubernetes_cluster_node_pool`  |

--------------------------------------------------------------------------------------------------------------------------------------
Kind: ConfigMap, Secret, Deploymnet, Service, ingress, StatefulSet, HorizontalPodAutoscaler, pod, ServiceAccount, StorageClass, PersistentVolume, PersistentVolumeClaim, PodDisruptionBudget,
      Role, RoleBinding, ClusterRole, ClusterRoleBinding, DaemonSet,  certificate, ReplicaSet, Job, CronJob, EndpointSlice, Lease, NetworkPolicy, LimitRange, ResourceQuota
pdb(pod distribution budget): A Pod Disruption Budget (PDB) allows you to limit the disruption to your application when its pods need to be rescheduled for some reason 
                              such as upgrades or routine maintenance work on the Kubernetes nodes.
hpa(HorizontalPodAutoscaler): A HorizontalPodAutoscaler (HPA for short) automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of 
                              automatically scaling the workload to match demand.Horizontal scaling means that the response to increased load is to deploy more Pods. 
                              This is different from vertical scaling, which for Kubernetes would mean assigning more resources (for example: memory or CPU) to the Pods that 
                              are already running for the workload.
nginx ingress controller:     A HorizontalPodAutoscaler (HPA for short) automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of 
                              automatically scaling the workload to match demand.Horizontal scaling means that the response to increased load is to deploy more Pods. 
                              This is different from vertical scaling, which for Kubernetes would mean assigning more resources (for example: memory or CPU) to the Pods 
                              that are already running for the workload.
nfs(network file sharing)(PersistentVolume)(PersistentVolumeClaim): is a request for storage by a user.It is similar to a Pod.
                              Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory).
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Azure: az aks get-credentials --resource-group aks-resource-group --name my-aks-cluster
aws: aws eks update-kubeconfig --region <AWS_REGION> --name <EKS_CLUSTER_NAME>  <-- to update your kubeconfig with your EKS cluster ~/.kube/config
gcp: gcloud container clusters get-credentials <CLUSTER_NAME> --region <REGION> --project <PROJECT_ID> 
-------------------------------------------------------------------------------------------------------------------------------------------
AZ--------Az--------AZ--------Az--------AZ--------Az--------AZ--------Az--------AZ--------Az--------AZ--------Az--------AZ--------Az--------AZ
az account set --subscription "<subscription-name-or-id>"     <--Switch between Azure subscriptions (your ‚Äúprojects‚Äù)
az account list --output table                                <--az account list --output table
az aks get-credentials --resource-group <resource-group-name> --name <aks-cluster-name>  <-- Once you‚Äôve set the right subscription, get the credentials for the cluster you want to manage
kubectl config get-contexts                                   <--View Available Contexts
kubectl config use-context <context-name>                     <--Switch Between AKS Clusters
kubectl config rename-context <old-name> <new-friendly-name>  <--Rename Contexts for Clarity
kubectl config rename-context aks-prod-cluster aks-prod
kubectl config use-context aks-prod
-------------------------------------------------------------------------------------------------------------------------------------
AWS-------EKS-------AWS-------EKS-------AWS-------EKS-------AWS-------EKS-------AWS-------EKS-------AWS-------EKS-------AWS-------EKS-------
Prerequisites
AWS CLI installed and configured (aws configure)
kubectl installed
eksctl (optional but helpful for EKS management)
aws configure  <--Ensure you are logged into AWS
aws configure --profile my-profile <--if using a specific profile
aws eks update-kubeconfig --region <AWS_REGION> --name <EKS_CLUSTER_NAME>  <-- to update your kubeconfig with your EKS cluster ~/.kube/config
eg:  aws eks update-kubeconfig --region us-west-2 --name my-cluster
#below command adds the cluster configuration to ~/.kube/config
aws eks update-kubeconfig --region us-west-2 --name my-cluster --profile my-profile   <--using a different AWS profile
kubectl config get-contexts  <--List available contexts
kubectl config current-context  <--Check the current context
kubectl config use-context <CONTEXT_NAME> <--Switch to a different EKS cluster
eg: kubectl config use-context arn:aws:eks:us-west-2:123456789:cluster/my-other-cluster
kubectl config set-context --current --namespace=my-namespace <--By default,kubectl uses the default namespace. Set a preferred namespace for your cluster
kubectl config view --minify | grep namespace  <--Verify the namespace

Q. How do you migrate an on-prem Kubernetes cluster to EKS?
Answer:Export workloads: Use kubectl get all -o yaml > backup.yaml.
Set up EKS: Create an EKS cluster with networking and IAM configurations.
Migrate Persistent Data: Use AWS EBS, EFS, or FSx for storage.
Deploy workloads: Apply kubectl apply -f backup.yaml to restore resources.
Update DNS & Load Balancers to point to EKS.

Q. How do you troubleshoot common EKS issues?
Answer:Pods not scheduling? Check 'kubectl describe pod' for resource limits or node capacity issues.
Networking issues? Check VPC CNI, security groups, and kubectl get svc.
IAM issues? Validate aws-auth ConfigMap and kubectl auth can-i.
Autoscaler not working? Check Cluster Autoscaler logs and ASG policies.
---------------------------------------------------------------------------------------------------------
aws eks update-kubeconfig  <--If you need to manage multiple EKS clusters, repeat aws eks update-kubeconfig for each cluster.
export KUBECONFIG=~/.kube/config:/path/to/another-kubeconfig  <--Alternatively, you can merge multiple kubeconfig files
kubectl cluster-info  <--Check cluster info
kubectl get nodes   <--List nodes
---------------------------------------------------------------------------------------------------------
GCP-----GKE------GCP-----GKE------GCP-----GKE------GCP-----GKE------GCP-----GKE------GCP-----GKE------GCP-----GKE------GCP-----GKE------
### (~/.kube/config) kubectl configuration to determine which Kubernetes cluster to deploy to.
###if you have multiple projects follow below steps
gcloud auth login   <--Ensure you are authenticated with GCP
gcloud auth activate-service-account --key-file=<key-file.json>      <--using service account credentials
gcloud container clusters get-credentials <CLUSTER_NAME> --region <REGION> --project <PROJECT_ID>   <--For each project and cluster, run
eg: 
gcloud container clusters get-credentials my-cluster-1 --region europe-west2 --project my-project-1
gcloud container clusters get-credentials my-cluster-2 --region us-central1 --project my-project-2  <--This updates ~/.kube/config with the new cluster contexts
kubectl config get-contexts    <---kubectl config get-contexts
----------------------------------------------------------------------------------------------------------
CURRENT   NAME                          CLUSTER            AUTHINFO            NAMESPACE
*         gke_my-project-1_europe-west2_my-cluster-1  gke_my-project-1_europe-west2_my-cluster-1  my-user
          gke_my-project-2_us-central1_my-cluster-2   gke_my-project-2_us-central1_my-cluster-2   my-user
----------------------------------------------------------------------------------------------------------
kubectl config use-context <context-name>    <--to switch to a different cluster
kubectl config use-context gke_my-project-2_us-central1_my-cluster-2   <--Switch context
#kubectl config current-context   <--can check your current cluster context
kubectl config set-context --current --namespace=my-namespace <--By default, kubectl uses the default namespace. To set a preferred namespace for a context
kubectl config set-context --current --namespace=dev-namespace <--To switch to another namespace (e.g., dev-namespace)
kubectl config set-context dev-context --cluster=<your-cluster-name> --user=<your-user> --namespace=dev-namespace
kubectl config use-context dev-context
kubectl config use-context prod-context        <--This way, you can switch between dev and prod context
kubectl config view --minify | grep namespace   <--Verify the namespace
----------------------------------------------------------------------------------------------------------
#Instead of modifying ~/.kube/config, you can use separate kubeconfig files.
export KUBECONFIG=/path/to/kubeconfig1         <--Set environment variable for a specific file
kubectl config use-context gke_my-project-1_europe-west2_my-cluster-1
export KUBECONFIG=~/.kube/config:/path/to/other/kubeconfig   <--Merge multiple kubeconfig files
kubectl cluster-info   <--Check which cluster you‚Äôre connected to
kubectl config current-context  <--Check active context
kubectl run hello-node --image=hello-node:v1 --port=8080           <--kubectl means cube control
kubectl describe pod
---------------------------------------------------------------------------------------------------------------------
kubectl describe pod my-app-abc
Name:         my-app-abc
Namespace:    default
Node:         node-1/10.0.0.1
...
Events:
| Type   | Reason    | Age | From               | Message                    |
|--------|-----------|-----|--------------------|----------------------------|
| Normal | Scheduled | 2h  | default-scheduler  | Successfully assigned...   |
-----------------------------------------------------------------------------------------------------------------------
kubectl delete pod hello-node
## Test to ensure the version you installed is up-to-date:
kubectl version --client
kubectl version
## By default, kubectl configuration is located at ~/.kube/config.Check that kubectl is properly configured by getting the cluster state:
kubectl cluster-info    <--It displays information about the Kubernetes cluster you‚Äôre currently connected to via your kubeconfig.
                        Output Includes: Kubernetes master (control plane) endpoint
                                         Core services like Kubernetes API Server, DNS, etc., and their URLs
üìã Example Output:
Kubernetes control plane is running at https://10.0.0.1:6443
CoreDNS is running at https://10.0.0.10:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
----------------------------------------------------------------------------------------------------------------------------
To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
----------------------------------------------------------------------------------------------------------------------------
kubectl get all          <--This command retrieves a list of all the primary Kubernetes resources in the current namespace.
It includes:Pods,Services,Deployments,ReplicaSets,StatefulSets,DaemonSets,Jobs,CronJobs (if available in your cluster version)
### Pods
| NAME               | READY | STATUS  | RESTARTS | AGE |
|--------------------|-------|---------|----------|-----|
| pod/my-app-abc     | 1/1   | Running | 0        | 5m  |

### Services
| NAME               | TYPE       | CLUSTER-IP | EXTERNAL-IP | PORT(S)   | AGE |
|--------------------|------------|------------|-------------|-----------|-----|
| service/kubernetes | ClusterIP  | 10.0.0.1   | <none>      | 443/TCP   | 1d  |

### Deployments
| NAME                    | READY | UP-TO-DATE | AVAILABLE | AGE |
|-------------------------|-------|------------|-----------|-----|
| deployment.apps/my-app  | 1/1   | 1          | 1         | 5m  |

### ReplicaSets
| NAME                              | DESIRED | CURRENT | READY | AGE |
|-----------------------------------|---------|---------|-------|-----|
| replicaset.apps/my-app-xyz-abc    | 1       | 1       | 1     | 5m  |
***It does not show resources across all namespaces.

***If you want to see everything in all namespaces, use:
kubectl get all --all-namespaces
### Pods Across All Namespaces
| NAMESPACE     | NAME                                   | READY | STATUS  | RESTARTS | AGE  |
|---------------|----------------------------------------|-------|---------|----------|------|
| default       | pod/my-app-abc                         | 1/1   | Running | 0        | 10m  |
| kube-system   | pod/coredns-558bd4d5db-xyz             | 1/1   | Running | 0        | 1d   |

### Services Across All Namespaces
| NAMESPACE | NAME                   | TYPE       | CLUSTER-IP | EXTERNAL-IP | PORT(S)  | AGE |
|-----------|------------------------|------------|------------|-------------|----------|-----|
| default   | service/kubernetes     | ClusterIP  | 10.0.0.1   | <none>      | 443/TCP  | 1d  |
---------------------------------------------------------------------------------------------------------------------------------

kubectl get nodes        <--To list all the nodes in the cluster
kubectl get pods                          <--Get all pods in current namespace
| NAME        | READY | STATUS  | RESTARTS | AGE |
|-------------|-------|---------|----------|-----|
| my-app-abc  | 1/1   | Running | 0        | 2h  |
kubectl get pods --all-namespaces         <-- It lists all pods running in all namespaces in your Kubernetes cluster.
                             #Output Includes:Pod name,Namespace,Status (Running, Pending, CrashLoopBackOff, etc.),Restarts count,Age
### Pods Across All Namespaces
| NAMESPACE     | NAME                           | READY | STATUS  | RESTARTS | AGE  |
|---------------|--------------------------------|-------|---------|----------|------|
| default       | my-app-abc                     | 1/1   | Running | 0        | 10m  |
| kube-system   | coredns-558bd4d5db-xyz         | 1/1   | Running | 0        | 1d   |

kubectl get pods --all-namespaces -o wide <-- It lists all pods in all namespaces, just like --all-namespaces, 
                                              with additional detailed info thanks to the -o wide flag.
                                              -o wide: Shows extra columns, such as:Node the pod is running on,Pod IP,Container image,Host IP,Nominated Node,Readiness gates (if applicable)
### Pods Across All Namespaces (Wide Output)
| NAMESPACE     | NAME                      | READY | STATUS  | RESTARTS | AGE  | IP           | NODE    | NOMINATED NODE | READINESS GATES |
|---------------|---------------------------|-------|---------|----------|------|--------------|---------|-----------------|------------------|
| default       | my-app-abc                | 1/1   | Running | 0        | 10m  | 10.42.0.15   | node-1  | <none>          | <none>           |
| kube-system   | coredns-558bd4d5db-xyz    | 1/1   | Running | 0        | 1d   | 10.42.0.5    | node-2  | <none>          | <none>           |

kubectl get pods -n dev                   <-- Fetches all pods in the dev namespace.

------------Namespace------------------------------------
kubectl create ‚Äìf namespace.yml --------->         command to create a namespace
kubectl get namespace ----------------->           list all the available namespace.
kubectl get namespace <Namespace name> ------->    get a particular namespace whose name is specified in the command
kubectl describe namespace <Namespace name> ---->  describe the complete details about the service.
kubectl delete namespace <Namespace name>------->  delete a particular namespace present in the cluster.
---------------------------------------------------------
kubectl create service nodeport nameOfservice(nginx) --tcp=80:80      <--name of service should be same as deploymnet
kubectl get service
kubectl create -h        <---help on creaing
kubectl create deployment nginx-deploy --image=nginx
kubectl get deployment
kubectl expose deployment/my-nginx
kubectl get pod  (or) kubectl get po        <--pod will be ready after deploymnet
kubectl get pods -l app=nginx
kubectl get replicaset   <--manages the replicas of a pod
kubectl get rs           <--manages the replicas of a pod
kubectl edit deployment nginx-deploy  <--you may edit version .. etc
kubectl get replicaset
kubectl logs podname                   <--get logs
kubectl create deployment mongo-deploy --image=mongo
kubectl create -f deploymnet.yaml       <--This command creates Kubernetes resources defined in the deploymnet.yaml file ‚Äî 
                                      typically a Deployment, but it could include other resources too (like Services, ConfigMaps, etc.), 
kubectl describe pod podname            <--get aditional information
kubectl exec -it podname -- /bin/bash   <--enter in to terminal for debugging
exit                                    <--to exit from container
kubectl get deployment                  <--check the name of deploymnet
kubectl delete deployment nameofdeploymnet
kubectl delete -f deployment.yml
kubectl delete -f service.yml
kubectl create deployment name image option1 option2 
kubectl apply -f config-file.yml        <--apply the configuration
kubectl describe service sevicename     <-- can see all service details like example port forwording..etc
kubectl get svc my-nginx
kubectl describe deployments
kubectl get pod -o wide                 <--get internal ip details of container
kubectl get deployment deploymnetname -o yaml > result.yml   <--can see etcd in yaml format
echo -n 'username' | base64             <-- to encript username or password to keep in secret
kubectl apply -f secret.yml
kubectl get secret
//kubectl delete deployments,svc my-nginx; kubectl create -f ./nginx-secure-app.yamL

#Delete node from cluster.
kubectl drain <node-name>
kubectl drain <node-name> --ignore-daemonsets --delete-local-data
kubectl delete node <node-name>
kubeadm reset
------------------------------------------------------------------------------------------------------------------------------------------
## üÜö `kubectl create` vs `kubectl apply`

| Feature                | `kubectl create`                                      | `kubectl apply`                                        |
|------------------------|-------------------------------------------------------|--------------------------------------------------------|
| üîÑ Operation Type      | Create-only (fails if the resource exists)            | Create or Update (idempotent)                          |
| üß† Smart Diffs         | ‚ùå No ‚Äì doesn't track changes                         | ‚úÖ Yes ‚Äì tracks and applies changes                    |
| üõë Resource Exists     | ‚ùå Fails with "already exists" error                  | ‚úÖ Updates the existing resource                        |
| üîÑ Use Case            | One-time creation                                     | Continuous delivery, GitOps, updates, rollbacks        |
| üßæ Command Example     | `kubectl create -f deployment.yaml`                   | `kubectl apply -f deployment.yaml`                     |
-------------------------------------------------------------------------------------------------------------------------------------------
kubeadm reset
echo 1 > /proc/sys/net/ipv4/ip_forward    <--ip_forward content with 1
kubeadm version
kubeadm upgrade plan
kubeadm upgrade apply v1.20.x

kubeadm upgrade node
kubeadm upgrade apply
kubeadm certs check-expiration
kubeadm certs renew apiserver --use-api &
kubectl certificate approve kubeadm-cert-kube-apiserver-ld526
systemctl daemon-reload
systemctl restart kubelet
---------Migration with node pools----------
kubectl cordon <Node name>
kubectl drain <Node nae> --force
gcloud container node-pools delete default-pool
--------
kubectl taint nodes backup1=backups-only:NoSchedule
---------------------------
minikube start
minikube status
--------------------------

Install kustomize
sudo -i
curl -s "https://raw.githubusercontent.com/\
> kubernetes-sigs/kustomize/master/hack/install_kustomize.sh"  | bash

mv kustomize /usr/local/bin/kustomize

uat only
kustomize build kustomize/overlay/dev/uat | kubectl apply -f -
pro only
kustomize build kustomize/overlay/dev/pro | kubectl apply -f -

* Note: By adding nfs to base, canary and default can no longer be deployed together. Deploy individually as above.
kustomize build kustomize/overlay/dev | kubectl apply -f -
------------------------------------------------------------------------------------------------------
resource "kubernetes_config_map" "ai-gateway-config" {
  metadata {
    name      = "ai-gateway-config"
    namespace = "api"
  }

  data = {
    # Define your config key-value pairs here
    "SPRING_PROFILES_ACTIVE" = "app10-b"
  }
}

resource "kubernetes_deployment" "ai-gateway" {
  metadata {
    name      = "ai-gateway"
    namespace = "api"
  }
  spec {
    replicas = 1

    selector {
      match_labels = {
        app = "ai-gateway"
      }
    }

    template {
      metadata {
        labels = {
          app = "ai-gateway"
        }
      }

      spec {
        container {
          name              = "ai-gateway"
          image             = "gcr.io/purplegrid/aigateway:app10-b"
          image_pull_policy = "Always"
          port {
            container_port = 80
          }
          env {
            # Inject config map data as environment variables
            name = "SPRING_PROFILES_ACTIVE"
            value_from {
              config_map_key_ref {
                name = "ai-gateway-config"
                key  = "SPRING_PROFILES_ACTIVE"
              }
            }
          }
          resources {
            limits = {
              cpu    = "2"
              memory = "8Gi"
            }
            requests = {
              cpu    = "1"
              memory = "6Gi"
            }
          }
        }
   ¬†¬†¬†}
¬†¬†¬†¬†}
¬†¬†}
}
------------------------------------------------------------------------------------------
apiVersion: cloud.google.com/v1
kind: BackendConfig
metadata:
  name: ai-gatewaybackendconfig
  namespace: api
spec:
  securityPolicy:
    name: "allowtheseguysonly"
  timeoutSec: 300
  healthCheck:
    checkIntervalSec: 15
    timeoutSec: 15
    healthyThreshold: 1
    unhealthyThreshold: 10
    type: HTTP
    requestPath: /aiservices/actuator/health
-----------------------------------------------------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: ai-gateway-service
  namespace: api
  annotations:
    cloud.google.com/backend-config: '{"default": "ai-gatewaybackendconfig"}'
spec:
  type: NodePort
  selector:
    app: ai-gateway
  ports:
  - protocol: TCP
    port: 80
  ¬†¬†targetPort:¬†80
------------------------------------------------------------------------------------------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app10-b-external-lb
  namespace: api
  annotations:
    #kubernetes.io/spec.ingressClassName: "gce"
    kubernetes.io/ingress.allow-http: "true"
    networking.gke.io/v1beta1.FrontendConfig: lb-frontend-config
spec:
  defaultBackend:
    service:
      name: jello-service
      port:
        number: 80
  rules:
  - http:
      paths:
      - path: /aigateway/*
        pathType: ImplementationSpecific
        backend:
          service:
            name: analytics-service
            port:
        ¬†¬†¬†¬†¬†¬†number:¬†80
---------------------------------------------------------------------------------------------------------------------------------
Kubernetes Kind, along with their purpose:
=================================================================================================================================
Additional Kubernetes Kinds You Might Need
Kind            Purpose
Service         Exposes a set of Pods as a network service
                Types: ClusterIP (default) ‚Üí Internal service within the cluster.
                       NodePort ‚Üí Exposes service on a node's static port.
                       LoadBalancer ‚Üí Uses a cloud provider‚Äôs external LB.
ServiceAccount  Provides an identity to Pods for interacting with the API server
Ingress         Manages external access to Services, provides HTTP/HTTPS routing.
Deployment      Manages and maintains Pods with ReplicaSets.
ConfigMap       Stores non-sensitive configuration data (e.g., environment variables, config files)
StorageClass    Defines storage types (e.g., SSD, HDD) for PersistentVolumes.
StatefulSet     Manages stateful applications (e.g., databases) ensuring stable identities and storage.
HorizontalPodAutoscaler(HPA):     Automatically scales Pods based on CPU or memory usage.
PersistentVolume(PV):             Provides a persistent storage resource.
PersistentVolumeClaim(PVC):       Requests storage from a PersistentVolume.
PodDisruptionBudget(PDB):         Ensures availability during voluntary disruptions.
Role & RoleBinding:               Grants permissions within a specific namespace.
ClusterRole & ClusterRoleBinding: Grants cluster-wide permissions.
DaemonSet       Ensures one Pod runs on every node.
Certificate     Manages TLS certificates for secure communication.
ExternalName    Maps to an external DNS name.
ReplicaSet	    Ensures a specific number of Pod replicas are running.
Job	          Runs a one-time task to completion.
CronJob	    chedules recurring Jobs like a cron job.
EndpointSlice   Improves service discovery and load balancing (replaces Endpoints).
Lease	          Helps coordinate leader election between Pods.
NetworkPolicy   Controls inbound and outbound traffic for Pods.
LimitRange	    Sets default CPU/memory limits per namespace.
ResourceQuota   Restricts resource consumption per namespace.
----------------------------------------------------------------------------------------------
1. Secret YAML File (secret.yaml)
apiVersion: v1
kind: Secret
metadata:
  name: my-secret
  namespace: default  # Change if needed
type: Opaque
data:
  username: dXNlcm5hbWU=   # echo -n "my-password" | base64
  password: cGFzc3dvcmQ=   ##To decode # echo "bXktcGFzc3dvcmQ=" | base64 --decode

2.How to Use the Secret in a Pod (pod.yaml)
The difference between kind: Pod and kind: Deployment in Kubernetes lies in their management, scalability, and fault tolerance.
A Pod is the smallest deployable unit in Kubernetes, representing one or more containers running together.
To use the secret in a Pod, define it in the env section:
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: nginx
    env:
    - name: DB_USERNAME
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: username
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: my-secret
          key: password

1Ô∏è‚É£ Service
Purpose: Exposes a set of Pods as a network service.
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    app: myapp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: ClusterIP  # Options: ClusterIP, NodePort, LoadBalancer
2Ô∏è‚É£ ServiceAccount
Purpose: Grants permissions to Pods to interact with the Kubernetes API.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: myapp-sa
3Ô∏è‚É£ Ingress
Purpose: Manages external HTTP/S access to Services.
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp-ingress
spec:
  rules:
    - host: myapp.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: myapp-service
                port:
                  number: 80
4Ô∏è‚É£ Deployment
Purpose: Manages Pods and ReplicaSets for stateless applications.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp-container
          image: nginx
          ports:
            - containerPort: 80
5Ô∏è‚É£ ConfigMap
Purpose: Stores non-sensitive configuration data.
apiVersion: v1
kind: ConfigMap
metadata:
  name: myapp-config
data:
  app.env: "production"
  log.level: "info"
6Ô∏è‚É£ StorageClass
Purpose: Defines storage parameters for dynamic volume provisioning.
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-storage
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
7Ô∏è‚É£ StatefulSet
Purpose: Manages stateful applications (e.g., databases).
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: myapp-db
spec:
  selector:
    matchLabels:
      app: myapp-db
  serviceName: "myapp-db-service"
  replicas: 2
  template:
    metadata:
      labels:
        app: myapp-db
    spec:
      containers:
        - name: db
          image: mysql:5.7
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: "password"
8Ô∏è‚É£ HorizontalPodAutoscaler (HPA)
Purpose: Automatically scales Pods based on CPU/memory usage.
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
9Ô∏è‚É£ PersistentVolume (PV)
Purpose: Represents a physical storage resource.
apiVersion: v1
kind: PersistentVolume
metadata:
  name: myapp-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: fast-storage
  hostPath:
    path: "/mnt/data"
üîü PersistentVolumeClaim (PVC)
Purpose: Requests storage from a PersistentVolume.
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myapp-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: fast-storage
1Ô∏è‚É£1Ô∏è‚É£ PodDisruptionBudget (PDB)
Purpose: Ensures a minimum number of Pods remain available during disruptions.
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: myapp
1Ô∏è‚É£2Ô∏è‚É£ Role
Purpose: Grants permissions within a specific namespace.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: myapp-role
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "watch"]
1Ô∏è‚É£3Ô∏è‚É£ RoleBinding
Purpose: Binds a Role to a user/service account.
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: myapp-rolebinding
  namespace: default
subjects:
  - kind: ServiceAccount
    name: myapp-sa
    namespace: default
roleRef:
  kind: Role
  name: myapp-role
  apiGroup: rbac.authorization.k8s.io
1Ô∏è‚É£4Ô∏è‚É£ ClusterRole
Purpose: Grants permissions across all namespaces.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: myapp-clusterrole
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
1Ô∏è‚É£5Ô∏è‚É£ ClusterRoleBinding
Purpose: Binds a ClusterRole to a user/service account.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: myapp-clusterrolebinding
subjects:
  - kind: ServiceAccount
    name: myapp-sa
    namespace: default
roleRef:
  kind: ClusterRole
  name: myapp-clusterrole
  apiGroup: rbac.authorization.k8s.io
1Ô∏è‚É£6Ô∏è‚É£ DaemonSet
Purpose: Ensures a Pod runs on every Node.
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: myapp-daemonset
spec:
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp-container
          image: busybox
          command: ["/bin/sh", "-c", "while true; do echo Hello; sleep 10; done"]
1Ô∏è‚É£7Ô∏è‚É£ Certificate (Using Cert-Manager)
Purpose: Manages SSL/TLS certificates in Kubernetes.
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: myapp-cert
spec:
  secretName: myapp-tls-secret
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  dnsNames:
    - myapp.example.com

1Ô∏è‚É£8Ô∏è‚É£  ReplicaSet
Purpose: Ensures a specific number of identical Pods are running.
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp-container
          image: nginx
          ports:
            - containerPort: 80
1Ô∏è‚É£9Ô∏è‚É£  Job
Purpose: Runs a one-time task until completion.
apiVersion: batch/v1
kind: Job
metadata:
  name: myapp-job
spec:
  template:
    spec:
      containers:
        - name: myapp-container
          image: busybox
          command: ["echo", "Hello, Kubernetes!"]
      restartPolicy: Never
2Ô∏è‚É£1Ô∏è‚É£ CronJob
Purpose: Runs Jobs on a schedule (like a cron job).
apiVersion: batch/v1
kind: CronJob
metadata:
  name: myapp-cronjob
spec:
  schedule: "*/5 * * * *"  # Runs every 5 minutes
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: myapp-container
              image: busybox
              command: ["echo", "Scheduled Task Executed"]
          restartPolicy: OnFailure
2Ô∏è‚É£4Ô∏è‚É£ EndpointSlice
Purpose: Improves service discovery and load balancing (replaces Endpoints).
apiVersion: discovery.k8s.io/v1
kind: EndpointSlice
metadata:
  name: myapp-endpointslice
  labels:
    kubernetes.io/service-name: myapp-service
addressType: IPv4
endpoints:
  - addresses:
      - 192.168.1.10
ports:
  - name: http
    port: 80
    protocol: TCP
2Ô∏è‚É£5Ô∏è‚É£ Lease
Purpose: Used for leader election in distributed systems.
apiVersion: coordination.k8s.io/v1
kind: Lease
metadata:
  name: myapp-lease
  namespace: default
spec:
  holderIdentity: "leader-1"
  leaseDurationSeconds: 30
2Ô∏è‚É£6Ô∏è‚É£ NetworkPolicy
Purpose: Controls inbound and outbound traffic for Pods.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-http
spec:
  podSelector:
    matchLabels:
      app: myapp
  policyTypes:
    - Ingress
  ingress:
    - from:
        - ipBlock:
            cidr: 192.168.1.0/24
      ports:
        - protocol: TCP
          port: 80
2Ô∏è‚É£7Ô∏è‚É£ LimitRange
Purpose: Sets default CPU/memory limits per namespace.
apiVersion: v1
kind: LimitRange
metadata:
  name: resource-limits
spec:
  limits:
    - default:
        cpu: "500m"
        memory: "512Mi"
      defaultRequest:
        cpu: "250m"
        memory: "256Mi"
      type: Container
2Ô∏è‚É£8Ô∏è‚É£ ResourceQuota
Purpose: Restricts resource consumption per namespace.
apiVersion: v1
kind: ResourceQuota
metadata:
  name: namespace-quota
spec:
  hard:
    pods: "10"
    requests.cpu: "2"
    requests.memory: "4Gi"
    limits.cpu: "4"
    limits.memory: "8Gi"
-------------------------------------------------------------------------------------------------------------
Creating an AKS Cluster and Deploying Resources
This guide will cover:
‚úÖ Creating an AKS cluster using Terraform
‚úÖ Deploying ConfigMaps & Secrets
‚úÖ Creating a Deployment
‚úÖ Exposing services using NodePort, ClusterIP, LoadBalancer
‚úÖ Setting up Ingress for traffic routing

1Ô∏è‚É£ Create AKS Cluster Using Terraform
Create a file aks.tf:

provider "azurerm" {
  features {}
}

resource "azurerm_resource_group" "aks_rg" {
  name     = "aks-resource-group"
  location = "East US"
}

resource "azurerm_kubernetes_cluster" "aks" {
  name                = "my-aks-cluster"
  location            = azurerm_resource_group.aks_rg.location
  resource_group_name = azurerm_resource_group.aks_rg.name
  dns_prefix          = "myaksdns"

  default_node_pool {
    name       = "agentpool"
    node_count = 2
    vm_size    = "Standard_B2s"
  }

  identity {
    type = "SystemAssigned"
  }
}

output "kube_config" {
  value     = azurerm_kubernetes_cluster.aks.kube_config_raw
  sensitive = true
}
Deploy AKS

terraform init
terraform apply -auto-approve
Get kubeconfig:
az aks get-credentials --resource-group aks-resource-group --name my-aks-cluster

2Ô∏è‚É£ Create ConfigMap & Secrets
ConfigMap (configmap.yaml)
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config
data:
  APP_ENV: "production"
  LOG_LEVEL: "info"
Apply:
kubectl apply -f configmap.yaml
Secret (secret.yaml)
apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  DATABASE_PASSWORD: cGFzc3dvcmQ=  # "password" base64 encoded
Apply:
kubectl apply -f secret.yaml

3Ô∏è‚É£ Create Deployment (deployment.yaml)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-container
          image: nginx
          ports:
            - containerPort: 80
          envFrom:
            - configMapRef:
                name: my-config
            - secretRef:
                name: my-secret
Apply:
kubectl apply -f deployment.yaml

4Ô∏è‚É£ Create Multiple Services
NodePort (nodeport-service.yaml)
apiVersion: v1
kind: Service
metadata:
  name: nodeport-service
spec:
  type: NodePort
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30080
Apply:
kubectl apply -f nodeport-service.yaml

ClusterIP (clusterip-service.yaml)
apiVersion: v1
kind: Service
metadata:
  name: clusterip-service
spec:
  type: ClusterIP
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
Apply:
kubectl apply -f clusterip-service.yaml

LoadBalancer (loadbalancer-service.yaml)
apiVersion: v1
kind: Service
metadata:
  name: loadbalancer-service
spec:
  type: LoadBalancer
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
Apply:
kubectl apply -f loadbalancer-service.yaml

5Ô∏è‚É£ Set Up Ingress
Install Ingress Controller
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
Ingress (ingress.yaml)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: myapp.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: clusterip-service
                port:
                  number: 80
Apply:
kubectl apply -f ingress.yaml

6Ô∏è‚É£ Verify Deployment
Check all resources:
kubectl get all

Get service details:
kubectl get svc

Describe ingress:
kubectl describe ingress my-ingress
üéØ Summary
‚úÖ Created an AKS cluster using Terraform
‚úÖ Deployed ConfigMaps & Secrets
‚úÖ Created a Deployment
‚úÖ Set up NodePort, ClusterIP, LoadBalancer services
‚úÖ Configured Ingress for external access

Would you like help customizing or troubleshooting any part? üöÄ
------------------------------------------------------------------------------------------
Sample production-ready Kubernetes manifests (secure defaults)

Below is a multi-document YAML you can apply as a template. It demonstrates production-minded 
defaults and common security best practices: non-root user, minimal capabilities, seccomp,
read-only root filesystem, resource requests/limits, probes, mounted ConfigMap/Secret 
(read-only & immutable), NetworkPolicy, and a PodDisruptionBudget. Replace placeholders 
(<...>) with your real values (image digest, domain, secrets).

# Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: myapp-prod
  labels:
    environment: production
---
# Immutable Secret (store sensitive values here)
apiVersion: v1
kind: Secret
metadata:
  name: myapp-secret
  namespace: myapp-prod
  annotations:
    # optional: annotate for your processes
    managed-by: "helm-or-manual"
immutable: true <--for production environments where configuration or secrets shouldn‚Äôt change unexpectedly.
type: Opaque <--default generic type for key-value secrets.Use case: Use when you just want to store arbitrary data such as passwords, tokens, or connection strings.
stringData:
  DATABASE_URL: "postgres://user:password@db.example.local:5432/mydb"
  API_KEY: "replace-with-real-key"
---
# ConfigMap for non-sensitive configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: myapp-config
  namespace: myapp-prod
data:
  APPLICATION_ENV: "production"
  LOG_LEVEL: "info"
---
# Deployment with security best-practices
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  namespace: myapp-prod
  labels:
    app: myapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1 <--If your Deployment has 3 replicas, during an update:Kubernetes 
                           can take down 1 old pod (so 2 remain running)Then start 1 new pod
                           Minimum of 2 pods will always be serving requests.
      maxSurge: 1 <--Kubernetes may temporarily run 1 extra pod(so 4 total if you had 3 replicas).
  template:
    metadata:
      labels:  <--Common use cases To select pods in:Service selectors,Deployment selectors,
                  NetworkPolicy, PodDisruptionBudget, HPA, etc.For grouping or filtering with:
                  #kubectl get pods -l app=myapp
                  #kubectl get pods -l environment=production
        app: myapp
      annotations: <--To record build or deployment metadata (Git commit, CI/CD build ID)..etc
        # recommend enabling pod-level seccomp if cluster supports it
        seccomp.security.alpha.kubernetes.io/pod: runtime/default
    spec:
      # Pod-level security context (file group, drop FS ownership surprises)
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 2000
      # Restrict scheduling to appropriate nodes if needed (nodeSelector / tolerations)
      containers:
      - name: myapp
        # pin image by digest in production
        image: ghcr.io/org/myapp@sha256:<PUT_REAL_DIGEST_HERE>
        imagePullPolicy: IfNotPresent  <--#Always	Pull the image every time the Pod starts ‚Äî even if it already exists on the node.
                                          #IfNotPresent	Pull the image only if it‚Äôs not already cached on the node.
                                          #Never	Never pull the image from a registry ‚Äî only use the local copy (must already exist).
        ports:
        - containerPort: 8080
          name: http
        # Environment from ConfigMap and Secret (Secrets mounted as env or files)
        envFrom:
        - configMapRef:
            name: myapp-config
        - secretRef:
            name: myapp-secret
        # Minimal privileges on process level
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
          runAsGroup: 1000
          capabilities:
            drop: ["ALL"]
          seccompProfile:
            type: RuntimeDefault
        # Resource requests and limits (tune for your app)
        resources:
          requests:
            cpu: "250m"
            memory: "256Mi"
          limits:
            cpu: "1000m"
            memory: "1Gi"
        # Liveness / readiness probes
        livenessProbe:
          httpGet:
            path: /healthz
            port: http
          initialDelaySeconds: 30
          periodSeconds: 20
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 3
        # Mount config and secrets read-only
        volumeMounts:
        - name: config
          mountPath: /etc/myapp/config
          readOnly: true
        - name: secret-vol
          mountPath: /etc/myapp/secret
          readOnly: true
      # Minimal volumes (only what is needed)
      volumes:
      - name: config
        configMap:
          name: myapp-config
      - name: secret-vol
        secret:
          secretName: myapp-secret
      # Do not run as privileged
      hostNetwork: false
      hostPID: false
      hostIPC: false
      # Optionally add imagePullSecrets if using private registry
      # imagePullSecrets:
      # - name: myregistry-cred
---
# Service (ClusterIP for internal apps; use LoadBalancer or Ingress for external)
apiVersion: v1
kind: Service
metadata:
  name: myapp-svc
  namespace: myapp-prod
  labels:
    app: myapp
spec:
  type: ClusterIP
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
---
# PodDisruptionBudget to keep availability during maintenance
apiVersion: policy/v1
kind: PodDisruptionBudget  <--Always keep at least 2 pods of this app running, even if nodes are being drained or updated.
metadata:
  name: myapp-pdb
  namespace: myapp-prod
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: myapp
---
# Simple NetworkPolicy: only allow traffic from namespace 'frontend' to this backend on TCP/80
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: myapp-only-from-frontend
  namespace: myapp-prod
spec:
  podSelector:
    matchLabels:
      app: myapp
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: frontend  # adjust to your frontend namespace label
    ports:
    - protocol: TCP
      port: 80
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
    ports:
    - protocol: TCP
      port: 53   # allow DNS; tailor egress rules tightly for your app


